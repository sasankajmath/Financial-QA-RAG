"""
LangGraph-based Multi-Agent System for Financial Q&A

Architecture:
- State: Holds conversation data (query, context, results, etc.)
- Tools: Retrieval tool (queries FAISS database)
- Nodes: Agent nodes (classify, retrieve, generate_answer)
- Edges: Conditional routing based on state

This replaces the orchestrator with a proper LangGraph state machine.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import TypedDict, Annotated, Sequence, Literal
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from utils.groq_helper import GroqHelper
from config import GROQ_MODEL, USE_MULTI_EMBEDDING

# ============================================================
# STATE DEFINITION
# ============================================================

class FinancialQAState(TypedDict):
    """State for the financial Q&A multi-agent system."""

    # Input
    query: str
    user_query: str

    # Agent 1: Problem Classification
    problem_type: Literal["PROBLEM_1", "PROBLEM_2", "PROBLEM_3"]
    problem_reasoning: str

    # Agent 2: Complexity Classification
    complexity: Literal["SIMPLE", "COMPLEX", "REASONING", "SUMMARIZATION"]
    info_type: Literal["numerical", "narrative", "mixed"]
    requires_decomposition: bool

    # Extraction
    company: str | None
    year: int | None

    # Agent 3: Retrieval
    retrieved_chunks: list
    context: str
    retrieval_strategy: str

    # Agent 4: Answer Generation
    answer: str
    has_answer: bool
    confidence: Literal["HIGH", "MEDIUM", "LOW"]

    # Metadata
    sources: list
    agent_trace: dict
    next_step: str


# ============================================================
# RETRIEVAL TOOL (Uses saved FAISS database)
# ============================================================

from rag_system.multi_retriever import MultiEmbeddingRetriever


def create_retrieval_tool(retriever):
    """
    Create a LangGraph-compatible retrieval tool that uses saved FAISS database.

    This is a TOOL that agents can call to retrieve relevant chunks.
    """

    def retrieval_tool(
        query: str,
        company: str = None,
        year: int = None,
        top_k: int = 10,
        embedding_type: str = "AUTO"
    ) -> dict:
        """
        Retrieve relevant chunks from FAISS database.

        Args:
            query: The search query
            company: Filter by company (AMZN/UBER)
            year: Filter by year (2019/2020/2021/2022)
            top_k: Number of chunks to retrieve
            embedding_type: AUTO, TEXT, or NUMERICAL

        Returns:
            Dictionary with retrieved chunks and metadata
        """
        print(f"üîç Retrieval Tool Called:")
        print(f"   Query: {query}")
        print(f"   Company: {company}, Year: {year}, Top-K: {top_k}")

        # Use the retriever (has cached FAISS database)
        results = retriever.search(
            query=query,
            company=company,
            year=year,
            top_k=top_k,
            embedding_type=embedding_type
        )

        # Format results for LangGraph
        chunks = []
        sources = []
        context_parts = []

        for i, r in enumerate(results):
            chunk = {
                'text': r['text'],
                'company': r['company'],
                'year': r['year'],
                'page': r['page'],
                'similarity': r['similarity']
            }
            chunks.append(chunk)
            sources.append({
                'company': r['company'],
                'year': r['year'],
                'page': r['page'],
                'similarity': r['similarity']
            })
            context_parts.append(f"[Source {i+1} - {r['company']} {r['year']} Pg {r['page']}]\n{r['text']}\n")

        return {
            'chunks': chunks,
            'sources': sources,
            'context': "\n".join(context_parts),
            'count': len(chunks)
        }

    return retrieval_tool


# ============================================================
# LANGGRAPH NODES (Agents)
# ============================================================

class FinancialQANodes:
    """All agent nodes for the LangGraph system."""

    def __init__(self, retriever, api_agent=None):
        self.retriever = retriever
        self.api_agent = api_agent
        self.groq = GroqHelper()

    # Node 1: Problem Type Classifier
    def problem_classifier_node(self, state: FinancialQAState) -> FinancialQAState:
        """Classify problem type (Problem 1/2/3)."""
        print("\n" + "="*70)
        print("üî∑ NODE 1: Problem Type Classification")
        print("="*70)

        query = state["user_query"]
        query_lower = query.lower()

        # Classify using LLM with structured output
        system_prompt = """Classify the query into problem type:

PROBLEM_1: Single company historical query from SEC 10-K
PROBLEM_2: Multi-company/multi-year comparison
PROBLEM_3: Real-time stock price query

Respond with ONLY: PROBLEM_1 or PROBLEM_2 or PROBLEM_3"""

        try:
            response = self.groq.client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Query: {query}"}
                ],
                temperature=0
            )

            result = response.choices[0].message.content.strip()
            problem_type = result if result in ["PROBLEM_1", "PROBLEM_2", "PROBLEM_3"] else "PROBLEM_1"

        except:
            # Fallback: keyword-based classification
            realtime_keywords = ['current', 'today', 'latest', 'now', 'stock price']
            if any(kw in query_lower for kw in realtime_keywords):
                problem_type = "PROBLEM_3"
            elif any(kw in query_lower for kw in ['compare', 'versus', 'vs', 'and']) + \
                     (1 if 'amazon' in query_lower else 0) + (1 if 'uber' in query_lower else 0) >= 2:
                problem_type = "PROBLEM_2"
            else:
                problem_type = "PROBLEM_1"

        reasoning = {
            "PROBLEM_1": "Single company, single year - RAG from 10-K",
            "PROBLEM_2": "Multi-company/multi-year - Complex RAG",
            "PROBLEM_3": "Real-time stock price - API call"
        }[problem_type]

        print(f"Problem Type: {problem_type}")
        print(f"Reasoning: {reasoning}\n")

        # Update state
        state["problem_type"] = problem_type
        state["problem_reasoning"] = reasoning
        state["next_step"] = "complexity_classifier"

        return state

    # Node 2: Complexity Classifier
    def complexity_classifier_node(self, state: FinancialQAState) -> FinancialQAState:
        """Classify query complexity."""
        print("üî∑ NODE 2: Complexity Classification")
        print("-"*70)

        query = state["user_query"]
        query_lower = query.lower()

        # Classify complexity
        reasoning_keywords = ['calculate', 'percentage', 'growth', 'rate', 'increase', 'decrease', 'margin']
        summary_keywords = ['summarize', 'overview', 'highlights', 'explain']
        comparison_keywords = ['compare', 'versus', 'vs', 'difference']
        numerical_keywords = ['revenue', 'sales', 'liabilities', 'assets', 'employees', 'income']

        has_reasoning = any(kw in query_lower for kw in reasoning_keywords)
        has_summary = any(kw in query_lower for kw in summary_keywords)
        has_comparison = any(kw in query_lower for kw in comparison_keywords)
        has_numerical = any(kw in query_lower for kw in numerical_keywords)

        if has_reasoning:
            complexity = "REASONING"
            requires_decomp = False
            info_type = "numerical"
        elif has_summary:
            complexity = "SUMMARIZATION"
            requires_decomp = False
            info_type = "mixed"
        elif has_comparison:
            complexity = "COMPLEX"
            requires_decomp = True
            info_type = "numerical" if has_numerical else "narrative"
        else:
            complexity = "SIMPLE"
            requires_decomp = False
            info_type = "numerical" if has_numerical else "narrative"

        print(f"Complexity: {complexity}")
        print(f"Information Type: {info_type}")
        print(f"Requires Decomposition: {requires_decomp}\n")

        # Extract entities
        company = None
        if 'amazon' in query_lower or 'amzn' in query_lower:
            company = 'AMZN'
        elif 'uber' in query_lower:
            company = 'UBER'

        year = None
        for y in [2019, 2020, 2021, 2022]:
            if str(y) in query:
                year = y

        print(f"Extracted - Company: {company}, Year: {year}\n")

        state["complexity"] = complexity
        state["info_type"] = info_type
        state["requires_decomposition"] = requires_decomp
        state["company"] = company
        state["year"] = year
        state["next_step"] = "check_problem_type"

        return state

    # Node 3: Entity Extraction
    def entity_extraction_node(self, state: FinancialQAState) -> FinancialQAState:
        """Extract company and year with fiscal year mapping."""
        print("üî∑ NODE 3: Entity Extraction & Year Mapping")
        print("-"*70)

        query = state["user_query"]
        query_lower = query.lower()

        # Extract company
        company = None
        if 'amazon' in query_lower or 'amzn' in query_lower:
            company = 'AMZN'
        elif 'uber' in query_lower:
            company = 'UBER'

        # Extract year
        year = None
        for y in [2019, 2020, 2021, 2022]:
            if str(y) in query:
                year = y

        # Apply fiscal to filing year mapping
        fiscal_to_filing = {2019: 2020, 2020: 2021, 2021: 2022}
        if year and year in fiscal_to_filing:
            fiscal_year = year
            filing_year = fiscal_to_filing[year]
            print(f"Fiscal Year: {fiscal_year}")
            print(f"Mapped to Filing Year: {filing_year}")
            year = filing_year

        print(f"Final - Company: {company}, Year: {year}\n")

        state["company"] = company
        state["year"] = year
        state["next_step"] = "retrieval"

        return state

    # Node 4: Retrieval
    def retrieval_node(self, state: FinancialQAState) -> FinancialQAState:
        """Retrieve relevant chunks from FAISS database."""
        print("üî∑ NODE 4: Retrieval (FAISS Database)")
        print("-"*70)

        # Determine retrieval strategy based on complexity
        if state["complexity"] == "SIMPLE":
            top_k = 5
            embedding = "NUMERICAL" if state["info_type"] == "numerical" else "TEXT"
        elif state["complexity"] == "COMPLEX":
            top_k = 15
            embedding = "NUMERICAL"
        elif state["complexity"] == "REASONING":
            top_k = 12
            embedding = "NUMERICAL"
        else:  # SUMMARIZATION
            top_k = 20
            embedding = "TEXT"

        print(f"Strategy: {state['complexity']} query")
        print(f"Top-K: {top_k}, Embedding: {embedding}")

        # Create retrieval tool and call it
        retrieval_tool = create_retrieval_tool(self.retriever)

        retrieval_result = retrieval_tool(
            query=state["query"],
            company=state["company"],
            year=state["year"],
            top_k=top_k,
            embedding_type=embedding
        )

        print(f"Retrieved {retrieval_result['count']} chunks\n")

        state["retrieved_chunks"] = retrieval_result["chunks"]
        state["context"] = retrieval_result["context"]
        state["sources"] = retrieval_result["sources"]
        state["retrieval_strategy"] = f"{state['complexity']}_{state['info_type']}"
        state["next_step"] = "answer_generation"

        return state

    # Node 5: Answer Generation
    def answer_generation_node(self, state: FinancialQAState) -> FinancialQAState:
        """Generate and validate final answer."""
        print("üî∑ NODE 5: Answer Generation & Validation")
        print("-"*70)

        # Select appropriate prompt based on complexity
        if state["complexity"] == "SIMPLE":
            prompt_type = "direct"
        elif state["complexity"] == "COMPLEX":
            prompt_type = "comparison"
        elif state["complexity"] == "REASONING":
            prompt_type = "analysis"
        else:  # SUMMARIZATION
            prompt_type = "summary"

        system_prompts = {
            "direct": """Extract the answer directly from context. Be concise and specific with numbers.""",
            "comparison": """Compare the values mentioned. Highlight differences and similarities clearly.""",
            "analysis": """Show your reasoning or calculation steps clearly. Provide final answer with methodology.""",
            "summary": """Synthesize key points from context. Use bullet points or structured format."""
        }

        system_prompt = f"""You are a financial analyst answering questions based on SEC 10-K filings.

CRITICAL RULES:
1. The context provided IS from the correct company and year - TRUST it
2. Extract exact numbers with units (million, billion, percentage)
3. {system_prompts[prompt_type]}

Context from {state.get('company', '')} {state.get('year', '')} 10-K:
{state['context']}

Question: {state['query']}

Answer:"""

        try:
            response = self.groq.client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[{"role": "system", "content": system_prompt}],
                temperature=0.1
            )

            answer = response.choices[0].message.content

            # Validate answer
            has_answer = any(char.isdigit() for char in answer) if state['info_type'] == 'numerical' else len(answer) > 50
            negative_phrases = ['could not find', 'does not contain', 'not available', 'no information']
            has_negative = any(phrase in answer.lower() for phrase in negative_phrases)

            confidence = "HIGH" if has_answer and not has_negative else "MEDIUM" if has_answer else "LOW"

            print(f"Answer Generated: {confidence} confidence")
            print(f"Validation: {'‚úì Valid' if has_answer else '‚úó Check needed'}\n")

            state["answer"] = answer
            state["has_answer"] = has_answer and not has_negative
            state["confidence"] = confidence

        except Exception as e:
            print(f"Error: {e}")
            state["answer"] = "I encountered an error generating the answer."
            state["has_answer"] = False
            state["confidence"] = "LOW"

        state["next_step"] = END
        return state


# ============================================================
# CONDITIONAL EDGES (Routing)
# ============================================================

def should_use_api(state: FinancialQAState) -> bool:
    """Check if query should go to API (real-time stock)."""
    return state.get("problem_type") == "PROBLEM_3"


def should_decompose(state: FinancialQAState) -> bool:
    """Check if query needs decomposition (complex comparison)."""
    return state.get("requires_decomposition", False)


# ============================================================
# BUILD LANGGRAPH
# ============================================================

def build_financial_qa_graph(retriever, api_agent=None):
    """
    Build the LangGraph state graph for financial Q&A.

    Args:
        retriever: MultiEmbeddingRetriever with cached FAISS database
        api_agent: Optional APIAgent for real-time stock queries

    Returns:
        Compiled LangGraph
    """
    # Initialize nodes
    nodes = FinancialQANodes(retriever, api_agent)

    # Create state graph
    graph = StateGraph(FinancialQAState)

    # Add nodes
    graph.add_node("problem_classifier", nodes.problem_classifier_node)
    graph.add_node("complexity_classifier", nodes.complexity_classifier_node)
    graph.add_node("entity_extraction", nodes.entity_extraction_node)
    graph.add_node("retrieval", nodes.retrieval_node)
    graph.add_node("answer_generation", nodes.answer_generation_node)

    # Add edges (flow)
    graph.set_entry_point("problem_classifier")

    graph.add_edge("problem_classifier", "complexity_classifier")
    graph.add_edge("complexity_classifier", "entity_extraction")
    graph.add_edge("entity_extraction", "retrieval")
    graph.add_edge("retrieval", "answer_generation")
    graph.add_edge("answer_generation", END)

    # Compile the graph
    app = graph.compile()

    print("="*70)
    print("LANGGRAPH FINANCIAL Q&A SYSTEM")
    print("="*70)
    print("\nNodes:")
    print("  1. problem_classifier ‚Üí Problem Type (1/2/3)")
    print("  2. complexity_classifier ‚Üí Complexity (Simple/Complex/Reasoning/Summary)")
    print("  3. entity_extraction ‚Üí Extract company/year with mapping")
    print("  4. retrieval ‚Üí Query FAISS database (saved embeddings)")
    print("  5. answer_generation ‚Üí Generate validated answer")
    print("\nTool:")
    print("  - retrieval_tool (uses cached FAISS database)")
    print("\n‚úÖ Graph compiled successfully!\n")

    return app


# ============================================================
# MAIN ENTRY POINT
# ============================================================

class LangGraphFinancialQA:
    """Main interface for the LangGraph-based financial Q&A system."""

    def __init__(self, retriever, api_agent=None):
        """
        Initialize the LangGraph system.

        Args:
            retriever: MultiEmbeddingRetriever with cached FAISS database
            api_agent: Optional APIAgent for real-time stock
        """
        print("\n" + "="*70)
        print("INITIALIZING LANGGRAPH MULTI-AGENT SYSTEM")
        print("="*70)

        self.retriever = retriever
        self.api_agent = api_agent
        self.graph = build_financial_qa_graph(retriever, api_agent)

        print("‚úÖ LangGraph system ready!\n")

    def visualize_graph(self):
        """
        Display the LangGraph state machine visualization.

        Shows the complete workflow with nodes and edges as a visual graph.
        """
        try:
            from IPython.display import Image, display

            # Generate the graph visualization
            graph_image = self.graph.get_graph().draw_mermaid_png()

            print("\n" + "="*70)
            print("LANGGRAPH WORKFLOW VISUALIZATION")
            print("="*70)
            print("\nüìä This graph shows the complete query processing pipeline:\n")
            print("   ‚Ä¢ Nodes: Agent processing steps")
            print("   ‚Ä¢ Edges: Flow between nodes")
            print("   ‚Ä¢ State: Data passed through the pipeline\n")

            display(Image(graph_image))

            print("\n" + "="*70)
            print("‚úÖ Graph visualization complete!")
            print("="*70 + "\n")

        except ImportError:
            print("\n‚ö†Ô∏è IPython display not available. Using text description instead:\n")
            self._print_text_description()
        except Exception as e:
            print(f"\n‚ö†Ô∏è Could not display graph: {e}")
            print("Using text description instead:\n")
            self._print_text_description()

    def _print_text_description(self):
        """Print a text description of the graph flow."""
        print("\n" + "="*70)
        print("LANGGRAPH WORKFLOW (Text Description)")
        print("="*70)
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  QUERY ‚Üí problem_classifier ‚Üí complexity_classifier        ‚îÇ
‚îÇ         ‚Üí entity_extraction ‚Üí retrieval ‚Üí answer_generation ‚îÇ
‚îÇ         ‚Üí END                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Nodes:
  1. problem_classifier     ‚Üí PROBLEM_1/2/3
  2. complexity_classifier  ‚Üí SIMPLE/COMPLEX/REASONING/SUMMARY
  3. entity_extraction      ‚Üí Company & Year with mapping
  4. retrieval              ‚Üí Query FAISS database
  5. answer_generation      ‚Üí Generate validated answer

Tool:
  - retrieval_tool (queries cached FAISS database)
""")

    def print_architecture(self):
        """Print the complete LangGraph architecture before query execution."""
        print("\n" + "‚ïî" + "‚ïê"*68 + "‚ïó")
        print("‚ïë" + " "*20 + "LANGGRAPH ARCHITECTURE" + " "*26 + "‚ïë")
        print("‚ïö" + "‚ïê"*68 + "‚ïù")

        print("\n‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*15 + "USER QUERY ENTERS SYSTEM" + " "*29 + "‚îÇ")
        print("‚îÇ" + " "*68 + "‚îÇ")
        print("‚îÇ  Example: \"What was Amazon's revenue in 2019?\"" + " "*20 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("\n" + "‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # STATE
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*22 + "INITIALIZE STATE" + " "*30 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FinancialQAState (TypedDict)                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  query: str           ‚Üí User's question                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  problem_type: None    ‚Üí Will be classified (PROBLEM_1/2/3)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  complexity: None      ‚Üí Will be classified                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  company: None         ‚Üí Will be extracted                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  year: None            ‚Üí Will be extracted + mapped            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  retrieved_chunks: []  ‚Üí Will be populated from FAISS          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  context: ""           ‚Üí Will be built from chunks             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  answer: ""            ‚Üí Will be generated by LLM              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  confidence: "LOW"     ‚Üí Will be scored (HIGH/MEDIUM/LOW)      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # NODE 1
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*8 + "NODE 1: PROBLEM CLASSIFIER AGENT" + " "*24 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT:  state["user_query"]                                         ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  PROCESS:                                                               ‚îÇ
‚îÇ  ‚Ä¢ Analyze query using LLM (llama-3.3-70b-versatile)                  ‚îÇ
‚îÇ  ‚Ä¢ Classify into problem type:                                         ‚îÇ
‚îÇ      - PROBLEM_1: Single company, single year (RAG from 10-K)        ‚îÇ
‚îÇ      - PROBLEM_2: Multi-company or multi-year comparison              ‚îÇ
‚îÇ      - PROBLEM_3: Real-time stock price (API call)                    ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  OUTPUT:                                                                 ‚îÇ
‚îÇ  ‚Üí state["problem_type"] = "PROBLEM_1" | "PROBLEM_2" | "PROBLEM_3"    ‚îÇ
‚îÇ  ‚Üí state["problem_reasoning"] = explanation                            ‚îÇ
‚îÇ  ‚Üí state["next_step"] = "complexity_classifier"                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # NODE 2
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*6 + "NODE 2: COMPLEXITY CLASSIFIER AGENT" + " "*24 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT:  state["user_query"]                                         ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  PROCESS:                                                               ‚îÇ
‚îÇ  ‚Ä¢ Analyze query complexity using keyword + LLM                        ‚îÇ
‚îÇ  ‚Ä¢ Classify into:                                                       ‚îÇ
‚îÇ      - SIMPLE: Direct fact retrieval                                   ‚îÇ
‚îÇ      - COMPLEX: Comparison or multi-part query                         ‚îÇ
‚îÇ      - REASONING: Calculation or analysis needed                       ‚îÇ
‚îÇ      - SUMMARIZATION: Overview or synthesis                            ‚îÇ
‚îÇ  ‚Ä¢ Determine information type: numerical | narrative | mixed          ‚îÇ
‚îÇ  ‚Ä¢ Extract company (AMZN | UBER) and year (2019-2022)                 ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  OUTPUT:                                                                 ‚îÇ
‚îÇ  ‚Üí state["complexity"] = "SIMPLE" | "COMPLEX" | "REASONING" | "SUMMARIZATION"‚îÇ
‚îÇ  ‚Üí state["info_type"] = "numerical" | "narrative" | "mixed"           ‚îÇ
‚îÇ  ‚Üí state["requires_decomposition"] = True | False                      ‚îÇ
‚îÇ  ‚Üí state["company"] = "AMZN" | "UBER" | None                           ‚îÇ
‚îÇ  ‚Üí state["year"] = 2019 | 2020 | 2021 | 2022 | None                   ‚îÇ
‚îÇ  ‚Üí state["next_step"] = "entity_extraction"                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # NODE 3
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*10 + "NODE 3: ENTITY EXTRACTION AGENT" + " "*22 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT:  state["user_query"], state["company"], state["year"]        ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  PROCESS:                                                               ‚îÇ
‚îÇ  ‚Ä¢ Extract company name from query                                      ‚îÇ
‚îÇ  ‚Ä¢ Extract fiscal year from query                                       ‚îÇ
‚îÇ  ‚Ä¢ APPLY FISCAL ‚Üí FILING YEAR MAPPING:                                  ‚îÇ
‚îÇ      Fiscal Year 2019 ‚Üí Filing Year 2020 (file: AMZN_2020.pdf)       ‚îÇ
‚îÇ      Fiscal Year 2020 ‚Üí Filing Year 2021 (file: AMZN_2021.pdf)       ‚îÇ
‚îÇ      Fiscal Year 2021 ‚Üí Filing Year 2022 (file: AMZN_2022.pdf)       ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  OUTPUT:                                                                 ‚îÇ
‚îÇ  ‚Üí state["company"] = "AMZN" | "UBER"                                  ‚îÇ
‚îÇ  ‚Üí state["year"] = filing_year (2020 | 2021 | 2022)                    ‚îÇ
‚îÇ  ‚Üí state["next_step"] = "retrieval"                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # NODE 4
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*18 + "NODE 4: RETRIEVAL AGENT" + " "*25 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT:  state["query"], state["company"], state["year"],             ‚îÇ
‚îÇ          state["complexity"], state["info_type"]                       ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  PROCESS:                                                               ‚îÇ
‚îÇ  ‚Ä¢ DETERMINE RETRIEVAL STRATEGY:                                        ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ      Complexity        | Embedding     | Top-K | Purpose              ‚îÇ
‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ      SIMPLE + numerical | NUMERICAL     | 5     | Fast, accurate       ‚îÇ
‚îÇ      SIMPLE + narrative | TEXT          | 5     | Semantic search      ‚îÇ
‚îÇ      COMPLEX            | NUMERICAL     | 15    | Extended + rerank    ‚îÇ
‚îÇ      REASONING          | NUMERICAL     | 12    | Context-aware        ‚îÇ
‚îÇ      SUMMARIZATION      | TEXT          | 20    | Broad retrieval      ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚Ä¢ CALL RETRIEVAL TOOL:                                                  ‚îÇ
‚îÇ      ‚Üí retrieval_tool(query, company, year, top_k, embedding_type)      ‚îÇ
‚îÇ      ‚Üí Queries cached FAISS database (no re-encoding!)                  ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  FAISS Database (Cached):                                                ‚îÇ
‚îÇ  ‚Ä¢ data/faiss_index/text_index.faiss (384-dim, MiniLM)                  ‚îÇ
‚îÇ  ‚Ä¢ data/faiss_index/numerical_index.faiss (768-dim, mpnet)             ‚îÇ
‚îÇ  ‚Ä¢ data/faiss_index/chunks.pkl (chunk metadata)                         ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  OUTPUT:                                                                 ‚îÇ
‚îÇ  ‚Üí state["retrieved_chunks"] = [chunk1, chunk2, ...]                    ‚îÇ
‚îÇ  ‚Üí state["context"] = formatted context with sources                    ‚îÇ
‚îÇ  ‚Üí state["sources"] = [{company, year, page, similarity}, ...]          ‚îÇ
‚îÇ  ‚Üí state["retrieval_strategy"] = "SIMPLE_numerical"                     ‚îÇ
‚îÇ  ‚Üí state["next_step"] = "answer_generation"                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # NODE 5
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*8 + "NODE 5: ANSWER GENERATION AGENT" + " "*22 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INPUT:  state["query"], state["context"], state["complexity"]        ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  PROCESS:                                                               ‚îÇ
‚îÇ  ‚Ä¢ Select prompt based on complexity:                                    ‚îÇ
‚îÇ      - direct: Extract specific fact                                    ‚îÇ
‚îÇ      - comparison: Highlight differences/similarities                   ‚îÇ
‚îÇ      - analysis: Show reasoning/calculation                             ‚îÇ
‚îÇ      - summary: Synthesize key points                                   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚Ä¢ Generate answer using LLM with context                                ‚îÇ
‚îÇ  ‚Ä¢ Validate answer:                                                      ‚îÇ
‚îÇ      ‚úì Check for numerical values (if numerical query)                  ‚îÇ
‚îÇ      ‚úì Check for negative phrases ("not found", "no information")       ‚îÇ
‚îÇ      ‚úì Assign confidence score                                          ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  OUTPUT:                                                                 ‚îÇ
‚îÇ  ‚Üí state["answer"] = generated answer text                              ‚îÇ
‚îÇ  ‚Üí state["has_answer"] = True | False                                   ‚îÇ
‚îÇ  ‚Üí state["confidence"] = "HIGH" | "MEDIUM" | "LOW"                      ‚îÇ
‚îÇ  ‚Üí state["next_step"] = END                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("‚îÇ"*70)
        print("‚îÇ"*70)
        print("‚îÇ"*70)

        # FINAL OUTPUT
        print("‚îå" + "‚îÄ"*68 + "‚îê")
        print("‚îÇ" + " "*23 + "FINAL OUTPUT" + " "*31 + "‚îÇ")
        print("‚îî" + "‚îÄ"*68 + "‚îò")
        print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RETURN FORMAT:                                                       ‚îÇ
‚îÇ  {                                                                     ‚îÇ
‚îÇ    'answer': "The net sales were $280,522 million...",                ‚îÇ
‚îÇ    'has_answer': True,                                                 ‚îÇ
‚îÇ    'confidence': "HIGH",                                               ‚îÇ
‚îÇ    'sources': [                                                        ‚îÇ
‚îÇ      {'company': 'AMZN', 'year': 2020, 'page': 22, 'similarity': 0.89}‚îÇ
‚îÇ    ],                                                                  ‚îÇ
‚îÇ    'method': 'LANGGRAPH_RAG',                                         ‚îÇ
‚îÇ    'agent_trace': {                                                    ‚îÇ
‚îÇ      'problem_type': 'PROBLEM_1',                                      ‚îÇ
‚îÇ      'complexity': 'SIMPLE',                                           ‚îÇ
‚îÇ      'company': 'AMZN',                                                ‚îÇ
‚îÇ      'year': 2020,                                                     ‚îÇ
‚îÇ      'retrieval_count': 5                                              ‚îÇ
‚îÇ    }                                                                   ‚îÇ
‚îÇ  }                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

        print("="*70)
        print("‚úÖ PIPELINE READY - QUERY EXECUTION STARTING")
        print("="*70)
        print()

    def route(self, query: str) -> dict:
        """
        Process a query through the LangGraph pipeline.

        Args:
            query: User's question

        Returns:
            Dictionary with answer and metadata
        """
        # Initialize state
        initial_state = FinancialQAState(
            query=query,
            user_query=query,
            problem_type=None,
            complexity=None,
            company=None,
            year=None,
            retrieved_chunks=[],
            context="",
            answer="",
            has_answer=False,
            confidence="LOW",
            sources=[],
            agent_trace={},
            next_step="problem_classifier"
        )

        # Check if it's an API query (Problem 3)
        query_lower = query.lower()
        api_keywords = ['current', 'today', 'latest', 'now', 'stock price']

        if any(kw in query_lower for kw in api_keywords):
            if self.api_agent:
                print("\n" + "="*70)
                print("üî∑ ROUTING TO API (Real-Time Stock)")
                print("="*70)
                print(f"Query: {query}\n")
                api_result = self.api_agent.answer(query)
                return {
                    'answer': api_result['answer'],
                    'has_answer': True,
                    'confidence': 'HIGH',
                    'sources': [],
                    'method': 'API_REALTIME'
                }

        # Run through LangGraph
        print("\n" + "="*70)
        print("üöÄ RUNNING LANGGRAPH PIPELINE")
        print("="*70)
        print(f"Query: {query}\n")

        # Invoke the graph
        final_state = self.graph.invoke(initial_state)

        # Return formatted result
        return {
            'answer': final_state['answer'],
            'has_answer': final_state['has_answer'],
            'confidence': final_state['confidence'],
            'sources': final_state['sources'],
            'method': 'LANGGRAPH_RAG',
            'agent_trace': {
                'problem_type': final_state['problem_type'],
                'complexity': final_state['complexity'],
                'company': final_state['company'],
                'year': final_state['year'],
                'retrieval_count': len(final_state['retrieved_chunks'])
            }
        }


if __name__ == "__main__":
    # Test the LangGraph system
    from rag_system.pdf_loader import load_pdfs
    from rag_system.chunking import chunk_documents
    from agents.api_agent import APIAgent

    print("Loading data...")
    pdf_path = os.path.join(os.path.dirname(os.getcwd()), "Assignment", "10-k_docs")
    documents = load_pdfs(pdf_path)
    chunks = chunk_documents(documents)

    print("Creating retriever...")
    from rag_system.multi_retriever import MultiEmbeddingRetriever
    retriever = MultiEmbeddingRetriever(chunks, use_cache=True)
    api_agent = APIAgent()

    # Create LangGraph system
    qa_system = LangGraphFinancialQA(retriever, api_agent)

    # Test
    test_queries = [
        "What was Amazon's revenue in 2019?",
        "What percentage of Amazon's revenue was in Q4 2019?",
        "Compare Amazon's net sales in 2019 vs 2021"
    ]

    for query in test_queries:
        result = qa_system.route(query)
        print(f"\nüìù Answer: {result['answer'][:200]}...")
        print(f"Confidence: {result['confidence']}")
        print("="*70)
        print()
