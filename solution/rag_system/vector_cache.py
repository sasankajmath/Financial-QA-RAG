"""
Vector Cache - Save and load FAISS indexes and embeddings to/from disk.

WHY:
- Creating embeddings takes 2-5 minutes every time
- FAISS indexes can be saved to disk
- Loading from disk takes only 5-10 seconds
- 90%+ speedup on subsequent runs!

This module handles persistent caching of:
1. FAISS indexes (binary files)
2. Embeddings (NumPy arrays)
3. Chunk metadata (pickle files)
4. Cache metadata (JSON - timestamps, model versions, etc.)
"""

import os
import json
import pickle
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

import faiss
import numpy as np


def get_cache_dir(cache_path: str = "data/faiss_index") -> str:
    """
    Get the cache directory path, creating it if it doesn't exist.
    
    Args:
        cache_path: Path to cache directory
        
    Returns:
        Absolute path to cache directory
    """
    cache_dir = Path(cache_path)
    cache_dir.mkdir(parents=True, exist_ok=True)
    return str(cache_dir)


def compute_pdf_hash(pdf_directory: str) -> str:
    """
    Compute a hash of all PDF files to detect changes.
    
    If PDFs are modified, cache should be invalidated.
    
    Args:
        pdf_directory: Directory containing PDF files
        
    Returns:
        MD5 hash of all PDF file contents
    """
    hasher = hashlib.md5()
    
    # Get all PDF files sorted for consistency
    pdf_files = sorted(Path(pdf_directory).rglob("*.pdf"))
    
    for pdf_file in pdf_files:
        # Add filename and file size to hash
        hasher.update(pdf_file.name.encode())
        hasher.update(str(pdf_file.stat().st_size).encode())
    
    return hasher.hexdigest()


def save_cache(
    index: faiss.Index,
    embeddings: np.ndarray,
    chunks: List[Dict],
    cache_dir: str = "data/faiss_index",
    model_name: str = "all-MiniLM-L6-v2",
    pdf_directory: str = "Assignment/10-k_docs",
    chunk_size: int = 1000,
    chunk_overlap: int = 200
) -> None:
    """
    Save FAISS index, embeddings, and chunks to disk.
    
    Args:
        index: FAISS index to save
        embeddings: NumPy array of embeddings
        chunks: List of chunk dictionaries with metadata
        cache_dir: Directory to save cache files
        model_name: Name of embedding model used
        pdf_directory: Directory containing source PDFs
        chunk_size: Size of chunks
        chunk_overlap: Overlap between chunks
    """
    cache_path = get_cache_dir(cache_dir)
    
    print(f"\nðŸ’¾ Saving cache to {cache_path}...")
    
    # 1. Save FAISS index
    index_file = os.path.join(cache_path, "faiss_index.index")
    faiss.write_index(index, index_file)
    print(f"   âœ“ Saved FAISS index ({os.path.getsize(index_file) / 1024 / 1024:.1f} MB)")
    
    # 2. Save embeddings as NumPy array
    embeddings_file = os.path.join(cache_path, "embeddings.npy")
    np.save(embeddings_file, embeddings)
    print(f"   âœ“ Saved embeddings ({os.path.getsize(embeddings_file) / 1024 / 1024:.1f} MB)")
    
    # 3. Save chunks metadata
    chunks_file = os.path.join(cache_path, "chunks.pkl")
    with open(chunks_file, "wb") as f:
        pickle.dump(chunks, f)
    print(f"   âœ“ Saved chunks metadata ({os.path.getsize(chunks_file) / 1024 / 1024:.1f} MB)")
    
    # 4. Save cache metadata
    metadata = {
        "created_at": datetime.now().isoformat(),
        "model_name": model_name,
        "num_chunks": len(chunks),
        "embedding_dimension": embeddings.shape[1],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "pdf_hash": compute_pdf_hash(pdf_directory),
        "total_embeddings": len(embeddings)
    }
    
    metadata_file = os.path.join(cache_path, "cache_metadata.json")
    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=2)
    print(f"   âœ“ Saved cache metadata")
    
    # Calculate total size
    total_size = sum([
        os.path.getsize(index_file),
        os.path.getsize(embeddings_file),
        os.path.getsize(chunks_file),
        os.path.getsize(metadata_file)
    ]) / 1024 / 1024
    
    print(f"\nâœ… Cache saved successfully! Total size: {total_size:.1f} MB")
    print(f"   Next run will load in ~5-10 seconds instead of 2-5 minutes!")


def load_cache(
    cache_dir: str = "data/faiss_index",
    pdf_directory: str = "Assignment/10-k_docs",
    validate: bool = True
) -> Optional[Dict[str, Any]]:
    """
    Load FAISS index, embeddings, and chunks from disk.
    
    Args:
        cache_dir: Directory containing cache files
        pdf_directory: Directory containing source PDFs (for validation)
        validate: Whether to validate cache consistency
        
    Returns:
        Dictionary with loaded data or None if cache doesn't exist/is invalid
        {
            "index": faiss.Index,
            "embeddings": np.ndarray,
            "chunks": List[Dict],
            "metadata": Dict
        }
    """
    cache_path = get_cache_dir(cache_dir)
    
    # Check if all required files exist
    index_file = os.path.join(cache_path, "faiss_index.index")
    embeddings_file = os.path.join(cache_path, "embeddings.npy")
    chunks_file = os.path.join(cache_path, "chunks.pkl")
    metadata_file = os.path.join(cache_path, "cache_metadata.json")
    
    required_files = [index_file, embeddings_file, chunks_file, metadata_file]
    
    if not all(os.path.exists(f) for f in required_files):
        print("ðŸ“­ No cache found. Will build embeddings from scratch...")
        return None
    
    print(f"\nðŸ“‚ Loading cache from {cache_path}...")
    
    try:
        # Load metadata first for validation
        with open(metadata_file, "r") as f:
            metadata = json.load(f)
        
        # Validate cache if requested
        if validate:
            # Check if PDFs have changed
            current_pdf_hash = compute_pdf_hash(pdf_directory)
            cached_pdf_hash = metadata.get("pdf_hash", "")
            
            if current_pdf_hash != cached_pdf_hash:
                print("âš ï¸ PDFs have been modified since cache was created.")
                print("   Cache is stale. Will rebuild embeddings...")
                return None
        
        # Load FAISS index
        index = faiss.read_index(index_file)
        print(f"   âœ“ Loaded FAISS index")
        
        # Load embeddings
        embeddings = np.load(embeddings_file)
        print(f"   âœ“ Loaded embeddings ({embeddings.shape[0]} chunks, {embeddings.shape[1]} dims)")
        
        # Load chunks
        with open(chunks_file, "rb") as f:
            chunks = pickle.load(f)
        print(f"   âœ“ Loaded {len(chunks)} chunks")
        
        # Verify consistency
        assert len(chunks) == len(embeddings), "Chunk count mismatch!"
        assert index.ntotal == len(embeddings), "Index size mismatch!"
        
        print(f"\nâœ… Cache loaded successfully!")
        print(f"   Created: {metadata['created_at']}")
        print(f"   Model: {metadata['model_name']}")
        print(f"   Speedup: ~90% faster than rebuilding from scratch!")
        
        return {
            "index": index,
            "embeddings": embeddings,
            "chunks": chunks,
            "metadata": metadata
        }
        
    except Exception as e:
        print(f"âŒ Error loading cache: {e}")
        print("   Will rebuild embeddings from scratch...")
        return None


def clear_cache(cache_dir: str = "data/faiss_index") -> None:
    """
    Delete all cache files.
    
    Use this to force rebuilding embeddings from scratch.
    
    Args:
        cache_dir: Directory containing cache files
    """
    cache_path = Path(cache_dir)
    
    if not cache_path.exists():
        print("ðŸ“­ No cache to clear.")
        return
    
    # Delete all files in cache directory
    deleted_count = 0
    for file in cache_path.glob("*"):
        if file.is_file():
            file.unlink()
            deleted_count += 1
    
    print(f"ðŸ—‘ï¸ Cleared {deleted_count} cache files from {cache_dir}")


def get_cache_info(cache_dir: str = "data/faiss_index") -> Optional[Dict]:
    """
    Get information about cached data without loading it.
    
    Args:
        cache_dir: Directory containing cache files
        
    Returns:
        Cache metadata dictionary or None if no cache exists
    """
    metadata_file = os.path.join(cache_dir, "cache_metadata.json")
    
    if not os.path.exists(metadata_file):
        return None
    
    with open(metadata_file, "r") as f:
        metadata = json.load(f)
    
    # Add file sizes
    cache_path = Path(cache_dir)
    total_size = sum(f.stat().st_size for f in cache_path.glob("*") if f.is_file())
    metadata["total_size_mb"] = total_size / 1024 / 1024
    
    return metadata


if __name__ == "__main__":
    # Test cache functions
    print("Testing vector cache module...\n")
    
    # Test 1: Get cache info
    print("Test 1: Get cache info")
    info = get_cache_info()
    if info:
        print(f"   Cache exists:")
        print(f"   - Created: {info['created_at']}")
        print(f"   - Model: {info['model_name']}")
        print(f"   - Chunks: {info['num_chunks']}")
        print(f"   - Size: {info['total_size_mb']:.1f} MB")
    else:
        print("   No cache found")
    
    print("\n" + "="*70 + "\n")
    
    # Test 2: Cache directory creation
    print("Test 2: Cache directory creation")
    cache_dir = get_cache_dir("data/faiss_index")
    print(f"   Cache directory: {cache_dir}")
    print(f"   Exists: {os.path.exists(cache_dir)}")
    
    print("\nâœ… Cache module tests complete!")
